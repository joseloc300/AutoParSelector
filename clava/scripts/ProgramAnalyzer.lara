import clava.autopar.Parallelize;
import clava.autopar.ParallelizeLoop;
import lara.Io;
import lara.util.LocalFolder;
import clava.ClavaJoinPoints;
import lara.cmake.CMaker;
import weaver.util.WeaverDataStore;
import lara.code.Timer;
import lara.metrics.ExecutionTimeMetric;
import weaver.Query;
import lara.Strings;

import CustomExecutionTimeMetric;
import CustomParallelize;
import IterCounting;
import LoopInfo;
import Utils;

aspectdef ProgramAnalyzer
	input
		originalParams,
		now
	end

	var params = Object.assign({}, originalParams);

	var start_time = performance.now();
	var AnalyzerResults = createBenchGroupResultsObj(params["loopGroupSizeLimit"]);

	var benchmarks = getBenchmarks(params);

	var autoParLoopItersFolder = Io.getAbsolutePath("extraIncludes");
	var autoParLoopItersHPath = Io.getAbsolutePath(autoParLoopItersFolder, "autopar_loop_iters.h"); //used to be path
	var autoParLoopItersCPath = Io.getAbsolutePath(autoParLoopItersFolder, "autopar_loop_iters.c"); // used to be path

	var loopItersFiles = {};
	loopItersFiles["autoParLoopItersFolder"] = autoParLoopItersFolder;
	loopItersFiles["autoParLoopItersHPath"] = autoParLoopItersHPath;
	loopItersFiles["autoParLoopItersCPath"] = autoParLoopItersCPath;

	var problemSizeFlags = Array.from(params["problemSizeFlags"]);
	if(problemSizeFlags.length == 0) {
		problemSizeFlags = [""];
	}

	println("Number of benchmarks found: " + benchmarks.length);
	println("Processing benchmarks");
	for (var benchmark of benchmarks) {
		println("Processing: " + benchmark.folder);
		
		for(var problemSizeFlag of problemSizeFlags) {

			if(problemSizeFlag != "") {
				println("Current problem size flag = " + problemSizeFlag);
			}
			
			Clava.pushAst();

			updateDynamicParams(params, benchmark, problemSizeFlag);
			rebuildCodeAst(params, params["xtraFiles"], params["xtraIncludeFolders"], false);

			var parLoops = null;
			var parLoopGroups = null;
			var benchmarkResults = null;
			var loopIterInfo = null;
			var removedPragmas = null;
			
			if(params["autoParSelectorFlags"]["readCache"]) {
				parLoops = getObjFromCache(params, "parLoops");
				
				if(checkEmptyParLoops(parLoops, params)) {
					Clava.popAst();
					continue;
				}

				if(params["autoParSelectorFlags"]["extractDynamicFeatures"] == 0) {
					loopIterInfo = getObjFromCache(params, "loopIterInfo");
				}
				parLoopGroups = getObjFromCache(params, "parLoopGroups");
				benchmarkResults = getObjFromCache(params, "benchmarkResults");
			}
			else {			
				if(params["foldersToGetExpectedC"].includes(benchmark.name))
					parLoops = getParLoopsFromExpectedOutput(params);
				else
					parLoops = getParLoops(params["functionFilters"]);

				if(checkEmptyParLoops(parLoops, params)) {
					Clava.popAst();
					continue;
				}
				
				removedPragmas = testPragmas(parLoops, params);

				//skip if all pragmas are removed
				if(checkEmptyParLoops(parLoops, params)) {
					Clava.popAst();
					continue;
				}

				if(params["autoParSelectorFlags"]["extractDynamicFeatures"]) {
					loopIterInfo = countLoopIterations(parLoops, params, loopItersFiles);
				}

				parLoopGroups = getParLoopGroups(parLoops, params["loopGroupSizeLimit"]);
				getLoopInfo(parLoops, loopIterInfo, params["autoParSelectorFlags"]["extractDynamicFeatures"]);

				benchmarkResults = createBenchResultsObj(benchmark, problemSizeFlag);
				addLoopInfo(parLoops, benchmarkResults);

				
				writeObjToCache(params, "parLoops", parLoops);
				writeObjToCache(params, "removedPragmas", removedPragmas);
				writeObjToCache(params, "loopIterInfo", loopIterInfo);
				writeObjToCache(params, "parLoopGroups", parLoopGroups);
				writeObjToCache(params, "benchmarkResults", benchmarkResults);
			}

			//if only calculating caches skip timing the benchmarks
			if(params["autoParSelectorFlags"]["onlyCalculateCaches"]) {
				Clava.popAst();
				continue;
			}
			
			AnalyzerResults["totalParLoops"] += Object.keys(parLoops).length;
			AnalyzerResults["totalVersions"] += parLoopGroups.length;
			AnalyzerResults["totalRuns"] += parLoopGroups.length * params["nRuns"];
			
			var version = 0;
			for (var loopGroup of parLoopGroups) {
				Clava.pushAst();
	
				addPragmas(loopGroup, parLoops);
	
				var measuresPar = addTimerPar(loopGroup, parLoops);
				var executorsPar = buildAndRun(params, version, params["nRuns"], true, params["autoParSelectorFlags"]["cleanBuilds"]);	
				addResults(benchmarkResults, measuresPar, executorsPar, version, loopGroup, parLoops);
				version++;
				Clava.popAst();
			}
			
			var mainLoopIds = getMainLoopIds(parLoops);
	
			var measuresSeq = addTimerSeq(mainLoopIds, parLoops);
			var executorsSeq = buildAndRun(params, version, params["nRuns"], false, params["autoParSelectorFlags"]["cleanBuilds"]);
			
			version = -1;
			addResults(benchmarkResults, measuresSeq, executorsSeq, version, loopGroup, parLoops);
			Clava.popAst();
			
			AnalyzerResults["benchmarks"].push(benchmarkResults);
			Io.writeJson(benchmark.outputFolder + problemSizeFlag + "/results.json", benchmarkResults);

			AnalyzerResults["totalBenchmarkVersions"] += 1;
		}

		AnalyzerResults["totalBenchmarks"] += 1;
	}
	var end_time = performance.now();
	AnalyzerResults["totalExecutionTimeInSec"] = (end_time - start_time) / 1000;
	
	Io.writeJson("." + folderSeparator + "results" + folderSeparator + params["benchGroupName"] + "_" + now.toISOString() + ".json", AnalyzerResults);
	println("Finished processing " + params["benchGroupName"]);

end
